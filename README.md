# Day 03: Natural Language Processing (NLP) ğŸ¤–ğŸ“ - Text Preprocessing & Vectorization

This practical session covers fundamental NLP techniques for text preprocessing and vectorization using Python's NLTK and scikit-learn libraries.

## ğŸ“ Files

1. **Text_Preprocessing.ipynb** - Complete text preprocessing pipeline âœ¨
2. **Text_Preprocessing_1.ipynb** - Basic text cleaning example ğŸ§¹
3. **Text_Vectorization.ipynb** - TF-IDF vectorization demonstration ğŸ“Š

---

## ğŸ”§ 1. Text Preprocessing Pipeline

### ğŸ“– Overview

Text preprocessing is essential for preparing raw text data for NLP tasks. This involves cleaning and normalizing text through multiple steps.

### ğŸ› ï¸ Key Techniques

#### **ğŸ”¤ Lowercasing**

Converts all text to lowercase for consistency.

```python
text = text.lower()
```

#### **ğŸš« Remove Punctuation**

Removes special characters and punctuation marks.

```python
text = text.translate(str.maketrans('', '', string.punctuation))
```

#### **âœ‚ï¸ Tokenization**

Splits text into individual words (tokens).

```python
tokens = word_tokenize(text)
```

#### **ğŸ—‘ï¸ Stop Words Removal**

Removes common words (like "the", "is", "and") that don't carry significant meaning.

```python
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word not in stop_words]
```

#### **ğŸŒ± Stemming**

Reduces words to their root form (e.g., "running" â†’ "run").

```python
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in filtered_tokens]
```

#### **ğŸ“š Lemmatization**

Converts words to their base dictionary form (e.g., "better" â†’ "good").

```python
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]
```

### ğŸ’» Example Code

```python
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize

text = "Hello! This is an Example showing, how text Pre-processing works better."

# Lowercasing
text = text.lower()

# Remove punctuation
text = text.translate(str.maketrans('', '', string.punctuation))

# Tokenization
tokens = word_tokenize(text)

# Remove stop words
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word not in stop_words]

# Stemming
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in filtered_tokens]

# Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]
```

---

## ğŸ“Š 2. TF-IDF Vectorization

### ğŸ“– Overview

TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects how important a word is to a document in a collection of documents.

### ğŸ¤” What is TF-IDF?

- **ğŸ“ˆ TF (Term Frequency)**: How often a word appears in a document
- **ğŸ” IDF (Inverse Document Frequency)**: How unique/rare a word is across all documents
- **â­ TF-IDF Score**: TF Ã— IDF - balances word frequency with uniqueness

### ğŸ’¡ Implementation

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Define sample documents
documents = [
    "Natural Language Processing is a subfield of artificial intelligence.",
    "It focuses on the interaction between computers and humans using natural language.",
    "TF-IDF is a technique used to evaluate the importance of words in a document."
]

# Initialize and fit the vectorizer
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Get feature names (vocabulary)
feature_names = vectorizer.get_feature_names_out()

# Convert to dense array for viewing
dense_matrix = tfidf_matrix.toarray()

# Display TF-IDF scores for each document
for i, doc in enumerate(dense_matrix):
    print(f"\nDocument {i+1} TF-IDF Scores:")
    for word, score in zip(feature_names, doc):
        print(f"{word}: {score:.4f}")
```

### âš™ï¸ How It Works

1. **ğŸ”¢ Vectorization**: Converts text documents into numerical vectors
2. **ğŸ“¦ Sparse Matrix**: Efficiently stores the TF-IDF values
3. **ğŸ“ Feature Names**: Unique words (vocabulary) across all documents
4. **ğŸ¯ Scoring**: Each word gets a TF-IDF score indicating its importance

---

## ğŸ› ï¸ Setup & Requirements

### ğŸ“¦ Install Dependencies

```bash
pip install nltk scikit-learn pandas
```

### â¬‡ï¸ Download NLTK Resources

```python
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
```

---

## ğŸ’¡ Key Takeaways

1. **ğŸ§¹ Text Preprocessing** prepares raw text for analysis by cleaning and normalizing it
2. **âš–ï¸ Stemming vs Lemmatization**: Stemming is faster but cruder; lemmatization is more accurate
3. **ğŸ¯ TF-IDF** helps identify important words by balancing frequency with uniqueness
4. **ğŸ”¢ Vectorization** converts text into numbers that machine learning algorithms can process

---

## ğŸš€ Use Cases

- **ğŸ˜Š Sentiment Analysis**: Preprocessing text before analyzing emotions
- **ğŸ“‚ Text Classification**: Categorizing documents by topic
- **ğŸ” Information Retrieval**: Search engines and document ranking
- **ğŸ’¬ Chatbots & NLP Applications**: Understanding and processing user input
